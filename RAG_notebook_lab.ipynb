{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymilvus in /opt/anaconda3/lib/python3.12/site-packages (2.5.7)\n",
      "Requirement already satisfied: setuptools>69 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (69.5.1)\n",
      "Requirement already satisfied: grpcio<=1.67.1,>=1.49.1 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (1.67.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (3.20.3)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (1.1.0)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (5.10.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (2.2.3)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pymilvus) (2.4.12)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from milvus-lite>=2.4.0->pymilvus) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas>=1.2.4->pymilvus) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pymupdf in /opt/anaconda3/lib/python3.12/site-packages (1.25.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sentence_transformers in /opt/anaconda3/lib/python3.12/site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (4.48.2)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (4.66.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (0.28.1)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence_transformers) (10.3.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.5.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence_transformers) (2025.1.31)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence_transformers) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: google-genai in /opt/anaconda3/lib/python3.12/site-packages (1.12.1)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (4.9.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (2.39.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (2.32.2)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (14.1)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-genai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.14.1->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.0.0->google-genai) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.2.2)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.14.1->google-genai) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymilvus\n",
    "%pip install pymupdf\n",
    "%pip install sentence_transformers\n",
    "%pip install google-genai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieval-Augmented Generation (RAG) service\n",
    "\n",
    "### Vector database setup\n",
    "\n",
    "In this part of laboratory, we will build a RAG service. It enhances the LLM text generation\n",
    "capabilities with context and information drawn from a knowledge base. Relevant textual information\n",
    "is found with vector search and appended to the prompt, resulting in less hallucinations and\n",
    "more precise, relevant answers.\n",
    "\n",
    "In such cases, we don't relaly need any additional capabilities like attributes filtering, ACID,\n",
    "JOINs or other Postgres-related advantages. Thus, we will use [Milvus](https://milvus.io/), a typical\n",
    "example of vector database. To generate embeddings, we will use\n",
    "[Silver Retriever model](https://huggingface.co/ipipan/silver-retriever-base-v1.1) from Sentence Transformers.\n",
    "It is based on HerBERT model for Polish language, and finetuned for retrieval of similar vectors.\n",
    "\n",
    "1. Start by setting up Milvus by using its Docker image. Docker Compose file is also conveniently\n",
    "   provided by its creators:\n",
    "```bash\n",
    "mkdir milvus_db\n",
    "cd milvus_db\n",
    "\n",
    "wget https://github.com/milvus-io/milvus/releases/download/v2.4.13-hotfix/milvus-standalone-docker-compose.yml -O docker-compose.yml\n",
    "```\n",
    "2. Run the database with `docker compose up -d`.\n",
    "3. Next code sections are quite interactive and will probably be easier to run inside a Jupyter\n",
    "   Notebook. Start it with `jupyter notebook`.\n",
    "4. Let's connect to the database. Milvus provides its own `pymilvus` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Run the database with `docker compose up -d`.\n",
    "3. Next code sections are quite interactive and will probably be easier to run inside a Jupyter\n",
    "   Notebook. Start it with `jupyter notebook`.\n",
    "4. Let's connect to the database. Milvus provides its own `pymilvus` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "host = \"localhost\"\n",
    "port = \"19530\"\n",
    "\n",
    "milvus_client = MilvusClient(\n",
    "    host=host,\n",
    "    port=port\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "5. Vector databases work quite similarly to document databases like e.g. MongoDB. We define\n",
    "   not a table, but a **collection** with specific **schema**, but conceptually it's a bit similar.\n",
    "   For each element, we have an ID, text, and its embedding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, DataType, CollectionSchema\n",
    "\n",
    "VECTOR_LENGTH = 768  # check the dimensionality for Silver Retriever Base (v1.1) model\n",
    "\n",
    "id_field = FieldSchema(name=\"id\", dtype=DataType.INT64, is_primary=True, description=\"Primary id\")\n",
    "text = FieldSchema(name=\"text\", dtype=DataType.VARCHAR, max_length=4096, description=\"Page text\")\n",
    "embedding_text = FieldSchema(\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=VECTOR_LENGTH, description=\"Embedded text\")\n",
    "\n",
    "fields = [id_field, text, embedding_text]\n",
    "\n",
    "schema = CollectionSchema(fields=fields, auto_id=True, enable_dynamic_field=True, description=\"RAG Texts collection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. To create a collection with the given schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rag_texts_and_embeddings']\n",
      "{'collection_name': 'rag_texts_and_embeddings', 'auto_id': True, 'num_shards': 1, 'description': 'RAG Texts collection', 'fields': [{'field_id': 100, 'name': 'id', 'description': 'Primary id', 'type': <DataType.INT64: 5>, 'params': {}, 'auto_id': True, 'is_primary': True}, {'field_id': 101, 'name': 'text', 'description': 'Page text', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 4096}}, {'field_id': 102, 'name': 'embedding', 'description': 'Embedded text', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}], 'functions': [], 'aliases': [], 'collection_id': 457644486125710553, 'consistency_level': 2, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': True, 'created_timestamp': 457647316695515140}\n"
     ]
    }
   ],
   "source": [
    "COLLECTION_NAME = \"rag_texts_and_embeddings\"\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    schema=schema\n",
    ")\n",
    "\n",
    "index_params = milvus_client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"embedding\", \n",
    "    index_type=\"HNSW\",\n",
    "    metric_type=\"L2\",\n",
    "    params={\"M\": 4, \"efConstruction\": 64}  # lower values for speed\n",
    ") \n",
    "\n",
    "milvus_client.create_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "# checkout our collection\n",
    "print(milvus_client.list_collections())\n",
    "\n",
    "# describe our collection\n",
    "print(milvus_client.describe_collection(COLLECTION_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "7. Now we are able to insert documents into put database. RAG is the most useful when information\n",
    "   is very specialized, niche, or otherwise probably unknown to the model or less popular. Let's\n",
    "   start with [\"IAB POLSKA Przewodnik po sztucznej inteligencji\"](https://www.iab.org.pl/wp-content/uploads/2024/04/Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf).\n",
    "   This part is inspired by [SpeakLeash](https://www.speakleash.org/) and one of their projects\n",
    "   [Bielik-how-to-start](https://github.com/speakleash/Bielik-how-to-start?tab=readme-ov-file) - \n",
    "   [Bielik_2_(4_bit)_RAG example](https://colab.research.google.com/drive/1ZdYsJxLVo9fW75uonXE5PCt8MBgvyktA?authuser=1). Bielik is the first Polish LLM, and you can also explore\n",
    "   other tutorials for its usage. Let's define some constants for a start:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data source and destination\n",
    "## the document origin destination from which document will be downloaded \n",
    "pdf_url = \"https://www.iab.org.pl/wp-content/uploads/2024/04/Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the document\n",
    "file_name = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.pdf\"\n",
    "\n",
    "## local destination of the processed document \n",
    "file_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska.json\"\n",
    "\n",
    "## local destination of the embedded pages of the document\n",
    "embeddings_json = \"Przewodnik-po-sztucznej-inteligencji-2024_IAB-Polska-Embeddings.json\"\n",
    "\n",
    "## local destination of all above local required files\n",
    "data_dir = \"./data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "8. Let's download the document into the `data_dir` directory:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "import os\n",
    "import requests\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "def download_pdf_data(pdf_url: str, file_name: str) -> None:\n",
    "    response = requests.get(pdf_url, stream=True)\n",
    "    with open(os.path.join(data_dir, file_name), \"wb\") as file:\n",
    "        for block in response.iter_content(chunk_size=1024):\n",
    "            if block:\n",
    "                file.write(block)\n",
    "\n",
    "download_pdf_data(pdf_url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "9. This is a lot of text, and in RAG we need to add specific fragments to the prompt.\n",
    "   To keep things simple, and number of vectors not too large, we will treat each page\n",
    "   as a separate **chunk** to vectorize and search for. Below, we paginate document and save each\n",
    "   page separately into a JSON file in format `{\"page\": page_number, \"text\": text_of_the_page}`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import json\n",
    "\n",
    "def extract_pdf_text(file_name, file_json):\n",
    "    document = fitz.open(os.path.join(data_dir, file_name))\n",
    "    pages = []\n",
    "\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        page_text = page.get_text()\n",
    "        pages.append({\"page_num\": page_num, \"text\": page_text})\n",
    "\n",
    "    with open(os.path.join(data_dir, file_json), \"w\") as file:\n",
    "        json.dump(pages, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "extract_pdf_text(file_name, file_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "10. Now we have texts, but we need vectors. We will use the model to embed text from each page\n",
    "    and save the result in out collection in Milvus. It's very easy if we first prepare a single\n",
    "    JSON file with all data. Its format is `{\"page\": page_num, \"embedding\": embedded_text}`.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "def generate_embeddings(file_json, embeddings_json, model):\n",
    "    pages = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    for page in data:\n",
    "        pages.append(page[\"text\"])\n",
    "\n",
    "    embeddings = model.encode(pages)\n",
    "\n",
    "    embeddings_paginated = []\n",
    "    for page_num in range(len(embeddings)):\n",
    "        embeddings_paginated.append({\"page_num\": page_num, \"embedding\": embeddings[page_num].tolist()})\n",
    "\n",
    "    with open(os.path.join(data_dir, embeddings_json), \"w\") as file:\n",
    "        json.dump(embeddings_paginated, file, indent=4, ensure_ascii=False)\n",
    "\n",
    "model_name = \"ipipan/silver-retriever-base-v1.1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(model_name, device=device)\n",
    "generate_embeddings(file_json, embeddings_json, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "11. Now we can easily insert the data into Milvus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_embeddings(file_json, embeddings_json, client=milvus_client):\n",
    "    rows = []\n",
    "    with open(os.path.join(data_dir, file_json), \"r\") as t_f, open(os.path.join(data_dir, embeddings_json), \"r\") as e_f:\n",
    "        text_data, embedding_data = json.load(t_f), json.load(e_f)\n",
    "        text_data =  list(map(lambda d: d[\"text\"], text_data))\n",
    "        embedding_data = list(map(lambda d: d[\"embedding\"], embedding_data))\n",
    "        \n",
    "        for page, (text, embedding) in enumerate(zip(text_data, embedding_data)):\n",
    "            rows.append({\"text\":text, \"embedding\": embedding})\n",
    "\n",
    "    client.insert(collection_name=\"rag_texts_and_embeddings\", data=rows)\n",
    "\n",
    "\n",
    "insert_embeddings(file_json, embeddings_json)\n",
    "\n",
    "# load inserted data into memory\n",
    "milvus_client.load_collection(\"rag_texts_and_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "12. Now let's do some semantic search!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Historia powstania\\nsztucznej inteligencji\\n7\\nW języku potocznym „sztuczny\" oznacza to, co\\njest \\nwytworem \\nmającym \\nnaśladować \\ncoś\\nnaturalnego. W takim znaczeniu używamy\\nterminu ,,sztuczny\\'\\', gdy mówimy o sztucznym\\nlodowisku lub oku. Sztuczna inteligencja byłaby\\nczymś (programem, maszyną) symulującym\\ninteligencję naturalną, ludzką.\\nSztuczna inteligencja (AI) to obszar informatyki,\\nktóry skupia się na tworzeniu programów\\nkomputerowych zdolnych do wykonywania\\nzadań, które wymagają ludzkiej inteligencji. \\nTe zadania obejmują rozpoznawanie wzorców,\\nrozumienie języka naturalnego, podejmowanie\\ndecyzji, uczenie się, planowanie i wiele innych.\\nGłównym celem AI jest stworzenie systemów,\\nktóre są zdolne do myślenia i podejmowania\\ndecyzji na sposób przypominający ludzki.\\nHistoria sztucznej inteligencji sięga lat 50. \\nXX wieku, kiedy to powstały pierwsze koncepcje\\ni modele tego, co mogłoby stać się sztuczną\\ninteligencją. Jednym z pionierów był Alan\\nTuring, który sformułował test Turinga, mający\\nna \\ncelu \\nocenę \\nzdolności \\nmaszyny \\ndo\\ninteligentnego \\nzachowania \\nna \\npoziomie\\nludzkim. Jednakże dopiero w latach 80. i 90.\\nnastąpił \\nprawdziwy \\nprzełom \\nw \\ndziedzinie\\nsztucznej \\ninteligencji \\ndzięki \\npostępowi \\nw\\ndziedzinie algorytmów uczenia maszynowego.\\nW wypadku sztucznej inteligencji mamy na\\nuwadze system, który realizowałby niektóre\\nfunkcje \\numysłu \\n– \\nczasami \\nw \\nsposób\\nprzewyższający funkcje naturalne (na przykład,\\naby był wolny od pomyłek przy liczeniu oraz\\ndefektów \\npamięci). \\nInteligencja \\njest \\nwła-\\nściwością umysłu. \\nSkłada się na nią szereg umiejętności, takich jak\\nzdolność do komunikowania, rozwiązywania\\nproblemów, uczenia się i dostosowywania do\\nsytuacji. \\nIstotna \\njest \\njednak \\numiejętność\\nrozumowania.\\nWspółczesne systemy sztucznej inteligencji są\\ninteligentne tylko w ograniczonym obszarze. \\nNa przykład komputer potrafi grać w szachy w\\ntaki \\nsposób, \\nże \\nwygrywa \\nz \\nszachowym\\narcymistrzem. W 1996 r. Deep Blue wygrał jedną\\npartię \\nszachów \\nz \\nGarry \\nKasparowem,\\nprzegrywając cały mecz wynikiem 4:2 (przy\\ndwóch remisach).\\nPóźniej Deep Blue został ulepszony i nie-\\noficjalnie \\nnazwany \\n„Deeper \\nBlue\". \\nZagrał\\nponownie z Kasparowem w maju 1997 roku.\\nMecz \\nskończył \\nsię \\nwynikiem \\n3½:2½ \\ndla\\nkomputera. W ten sposób Deep Blue stał się\\npierwszym systemem komputerowym, który\\nwygrał z aktualnym mistrzem świata w meczu\\nze standardową kontrolą czasu.\\nŹródło: Midjourney – obraz wygenerowany przez AI\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def search(model, query, client=milvus_client):\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "    result = client.search(\n",
    "        collection_name=\"rag_texts_and_embeddings\", \n",
    "        data=[embedded_query], \n",
    "        limit=1,\n",
    "        search_params={\"metric_type\": \"L2\"},\n",
    "        output_fields=[\"text\"]\n",
    "    )\n",
    "    return result\n",
    "\n",
    "\n",
    "result = search(model, query=\"Czym jest sztuczna inteligencja\")\n",
    "result[0][0].entity.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "However, **this is not yet RAG!**. This is just searching through our embeddings, without\n",
    "any LLM or generation. Many companies rely on external LLMs used via API, due to easy setup,\n",
    "good scalability, and low cost. We will follow this trend here and use Google Gemini API\n",
    "to generate answer with RAG.\n",
    "\n",
    "### Gemini API integration\n",
    "\n",
    "Gemini API is free to use, with rate limit for the [free version of the API](https://ai.google.dev/pricing?hl=pl#1_5flash).\n",
    "Specifically, we have 1500 RPM, i.e. requests per minute.\n",
    "\n",
    "1. Get the API key from the Gemini API. Go to [model info](https://ai.google.dev/pricing?hl=pl#1_5flash), and click \"Try it now in Google AI Studio\".\n",
    "![Try it now](MODEL_INFO_SS.png)\n",
    "\n",
    "2. You will be redirected to the Google AI Studio. Click \"Get API Key\". \n",
    "![Get API Key](GOOGLE_AI_STUDIO_SS.png)\n",
    "\n",
    "3. Then click \"Create API Key\" and copy the key.\n",
    "![Create API Key](API_KEYS_SS.png)\n",
    "\n",
    "4. Save the key in the environment variable. This is a **secret**, like any other API key, and **must never be shared**!\n",
    "```bash\n",
    "export GEMINI_API_KEY=your_api_key\n",
    "```\n",
    "\n",
    "5. Let's prepare the function that will call Google API and generate our response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "\n",
    "GEMINI_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "gemini_client = genai.Client(api_key=GEMINI_KEY)\n",
    "\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "def generate_response(prompt: str):\n",
    "    try:\n",
    "        # Send request to Gemini 2.0 Flash API and get the response\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=MODEL,\n",
    "            contents=prompt,\n",
    "        )\n",
    "        return response.text \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Now we can fully integrate everything into a RAG system. Fill the function below that will\n",
    "   augment the prompt with knowledge from Milvus, and then use the LLM to generate an answer\n",
    "   based on that context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "def build_prompt(context: str, query: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "    Using data provided in context section, answer the question.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question:\n",
    "    {query}\n",
    "    \n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "    \n",
    "\n",
    "def rag(model, query: str) -> str:\n",
    "    \n",
    "    result = search(model, query=\"Czym jest sztuczna inteligencja\")\n",
    "    context = result[0][0].entity.text\n",
    "\n",
    "    prompt = build_prompt(context, query)\n",
    "\n",
    "    response = generate_response(prompt)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Test the RAG system with a few sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Badania nad sztuczną inteligencją rozpoczęto w latach 50. XX wieku.\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(model, \"W którym roku rozpoczęto badania nad szutuczną inteligencją?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sztuczna inteligencja to obszar informatyki, który skupia się na tworzeniu programów komputerowych zdolnych do wykonywania zadań wymagających ludzkiej inteligencji. Głównym celem AI jest stworzenie systemów, które myślą i podejmują decyzje w sposób przypominający ludzki.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(model, \"W dwóch zdaniach odpowiedz czym jest sztuczna inteligencja\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Przepraszam, nie mogę odpowiedzieć na to pytanie. Podany tekst nie zawiera informacji na temat tego, co będziesz jadł jutro na obiad.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(model, \"Co będziemy jeść jutro na obiad?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
